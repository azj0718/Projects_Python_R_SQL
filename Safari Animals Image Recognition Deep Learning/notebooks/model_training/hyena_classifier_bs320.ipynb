{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYeoUwM9NWqr",
        "outputId": "cc9a0213-e83d-4459-8546-80d880e52faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 585 kB 12.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 419 kB 42.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 141 kB 58.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pytorch_lightning"
      ],
      "id": "oYeoUwM9NWqr"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3Jllgy4t2XU",
        "outputId": "5fb2c2e9-e287-4cda-e2bd-71036d21dcfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 34 kB/s \n",
            "\u001b[K     |████████████████████████████████| 199 kB 52.5 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark"
      ],
      "id": "H3Jllgy4t2XU"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "edcade47"
      },
      "outputs": [],
      "source": [
        "## Databricks notebook source\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.multiprocessing import cpu_count\n",
        "import pytorch_lightning as pl\n",
        "from torch.multiprocessing import cpu_count\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "from pyspark.sql.functions import col\n",
        "from joblib import dump, load\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "id": "edcade47"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpi0VhieNlDT",
        "outputId": "30a9a874-c13b-4d65-c38a-93ce15c3c800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive to upload datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Vpi0VhieNlDT"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11605b58",
        "outputId": "f212cdcc-7995-4d2b-f480-b01096fcb155"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Load the embeddings from the previously trained SimCLR model into a new resnet model for inference\n",
        "resnet18_new = torchvision.models.resnet18()\n",
        "backbone_new = nn.Sequential(*list(resnet18_new.children())[:-1])\n",
        "ckpt = torch.load('/content/drive/My Drive/individual_rec_models/individ_rec_modelsandhelpers/simclrresnet18embed.pth')\n",
        "backbone_new.load_state_dict(ckpt['resnet18_parameters'])"
      ],
      "id": "11605b58"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "95f7a86b"
      },
      "outputs": [],
      "source": [
        "def generate_embeddings(model, device, dl):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for im, label in dl:\n",
        "          im = im.to(device)\n",
        "          embed = model(im).flatten(start_dim=1)\n",
        "          embeddings.append(embed)\n",
        "          labels.append(label)\n",
        "    embeddings = torch.cat(embeddings, 0)\n",
        "    embeddings = embeddings.cpu()\n",
        "    embeddings = normalize(embeddings)\n",
        "    embeddings = np.array(embeddings)\n",
        "    labels = np.array([x.item() for x in labels])\n",
        "    return embeddings, labels"
      ],
      "id": "95f7a86b"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5bfcbfed"
      },
      "outputs": [],
      "source": [
        "# Define a transformation pipeline to be used to generate the embeddings\n",
        "class_transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((224, 224)),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "        ])"
      ],
      "id": "5bfcbfed"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c91db622"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"A custom class to create the transformed datasets\"\"\"\n",
        "    \n",
        "    def __init__(self, root, folder, transform, mappings=None):\n",
        "        super().__init__()\n",
        "        # The path to the root directory and the specific folder in it\n",
        "        self.root = root\n",
        "        self.folder = folder\n",
        "        # The images to be included\n",
        "        self.img_dir = glob.glob(os.path.join(self.root, self.folder)+'/*/*.jpg')\n",
        "        \n",
        "        # Get the unique animal identifiers\n",
        "        self.animal_ids = self._get_animal_ids()\n",
        "        \n",
        "        if mappings:\n",
        "            # Create the mapping of animal string ids to integer values, using the passed mappings\n",
        "            # the passed mappings will be the same as the one produced by the training set\n",
        "            self.class_to_idx = mappings\n",
        "        else:\n",
        "            # Create a list of integers for the labels\n",
        "            self.animal_int_labels = [x for x in range(len(self.animal_ids))]\n",
        "            self.class_to_idx = dict(zip(self.animal_ids, self.animal_int_labels))\n",
        "        \n",
        "        # Define the preprocessing function\n",
        "        self.transform = transform\n",
        "    \n",
        "    def _get_animal_ids(self):\n",
        "        \"\"\"Create a set of unique animal ids in string form\"\"\"\n",
        "        animal_ids = os.listdir(self.root)\n",
        "        return list(animal_ids)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.img_dir)\n",
        "    \n",
        "    def __getiteminternal__(self, idx):\n",
        "        # Get the path to the image\n",
        "        img_path = self.img_dir[idx]\n",
        "        im = Image.open(img_path).convert('RGB')\n",
        "        new_im = self.transform(im)\n",
        "        # Get the animal name from the full path\n",
        "        animal_name = img_path.split('/')[-2]\n",
        "        label = self.class_to_idx[animal_name]\n",
        "        # Return the two processed versions of the input image and a dummy label\n",
        "        return new_im, label\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.__getiteminternal__(idx)"
      ],
      "id": "c91db622"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ad04f88b"
      },
      "outputs": [],
      "source": [
        "# Create the datasets and custom dataloaders - set the batch size to 1 so that all examples are used\n",
        "data_dir = '/content/drive/My Drive/hyena.coco/processed/'\n",
        "training_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=class_transform)\n",
        "val_dataset = CustomDataset(data_dir, 'val', transform=class_transform, mappings=training_dataset.class_to_idx)"
      ],
      "id": "ad04f88b"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1e0b207e"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders of the training, validation, and test sets\n",
        "train_dl = DataLoader(training_dataset, batch_size=1, num_workers=4, shuffle=False)\n",
        "val_dl = DataLoader(val_dataset, batch_size=1, num_workers=4, shuffle=False)"
      ],
      "id": "1e0b207e"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nwAmJU7xiWYB"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "nwAmJU7xiWYB"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cfc65107"
      },
      "outputs": [],
      "source": [
        "# Generate the embeddings for the train, validation, and test datasets\n",
        "train_embeddings, train_labels = generate_embeddings(backbone_new, device, train_dl)"
      ],
      "id": "cfc65107"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d366a63d"
      },
      "outputs": [],
      "source": [
        "val_embeddings, val_labels = generate_embeddings(backbone_new,device,val_dl)"
      ],
      "id": "d366a63d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Use K-fold cross validation to train the classifier since some classes will only have 1 example\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "\n",
        "# Define the parameter grid for the KNN model to be searched\n",
        "knn_param_grid = [{'pca__n_components': [0.8, 0.9, 0.95, 0.99],\n",
        "                   'KNN__n_neighbors': [1, 3, 5, 10], \n",
        "                   'KNN__weights': ['uniform', 'distance'], \n",
        "                   'KNN__metric': ['euclidean', 'manhattan', 'cosine']}]\n",
        "\n",
        "# Define the pipe object to use in Grid Search\n",
        "pipe_knn = Pipeline([('scaler', StandardScaler()), ('pca', PCA()), ('KNN', KNeighborsClassifier())])\n",
        "\n",
        "# Create a grid search object and parameters to be searched\n",
        "knn_grid_search = GridSearchCV(estimator=pipe_knn, param_grid=knn_param_grid, scoring='accuracy', cv=cv)\n",
        "\n",
        "# Fit the data to the training data\n",
        "knn_grid_search.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Get the best estimator from the grid search results\n",
        "clf = knn_grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "MXDefAkAEOTR"
      },
      "id": "MXDefAkAEOTR",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fHb628S9izMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17848152-c761-467f-db35-955570d20328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN validation accuracy score: 0.45806451612903226\n"
          ]
        }
      ],
      "source": [
        "# Fit a KNN classifier to the \n",
        "knn_val_preds= clf.predict(val_embeddings)\n",
        "print(f'KNN validation accuracy score: {accuracy_score(knn_val_preds, val_labels)}')"
      ],
      "id": "fHb628S9izMq"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AT9F-ugijOr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450ed544-fba6-4715-832b-de02cbd1c0f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/individual_rec_models/Crocuta_crocuta_knn.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Save the model\n",
        "dump(clf, '/content/drive/My Drive/individual_rec_models/Crocuta_crocuta_knn.joblib')"
      ],
      "id": "AT9F-ugijOr4"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XCD5VV_Y_ZYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2df703e-0ae3-4f68-cea7-0f74023e35d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/individual_rec_models/hyena_id_map.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Get the integer to string mapping and save as a joblib file\n",
        "# Swap the keys and values in the name map to enable looking up the string ids\n",
        "res = dict((v,k) for k,v in training_dataset.class_to_idx.items())\n",
        "# Save the leopard id look up dict for use with the Flask API\n",
        "dump(res, '/content/drive/My Drive/individual_rec_models/hyena_id_map.joblib')"
      ],
      "id": "XCD5VV_Y_ZYE"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "hyena_classifier_bs320.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}